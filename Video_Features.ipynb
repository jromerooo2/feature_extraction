{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jromerooo2/feature_extraction/blob/main/Video_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Features using (2+1)D.\n",
        "TimeSformer doesn't support the feature extraction, refer to: https://github.com/facebookresearch/TimeSformer/issues/31"
      ],
      "metadata": {
        "id": "DQupZaGNI_pm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn1MrAwGGkyY",
        "collapsed": true,
        "outputId": "fe83166f-20c2-4bf4-8daf-4be41f81f826"
      },
      "source": [
        "! git clone https://github.com/v-iashin/video_features.git\n",
        "! pip install omegaconf==2.0.6"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'video_features'...\n",
            "remote: Enumerating objects: 1299, done.\u001b[K\n",
            "remote: Counting objects: 100% (406/406), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 1299 (delta 251), reused 311 (delta 203), pack-reused 893\u001b[K\n",
            "Receiving objects: 100% (1299/1299), 288.63 MiB | 17.09 MiB/s, done.\n",
            "Resolving deltas: 100% (668/668), done.\n",
            "Updating files: 100% (177/177), done.\n",
            "Collecting omegaconf==2.0.6\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (4.6.3)\n",
            "Installing collected packages: omegaconf\n",
            "Successfully installed omegaconf-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq-_J-msB8Px",
        "outputId": "8325a932-014d-4e56-84dd-0fda406d863e"
      },
      "source": [
        "%cd video_features"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/video_features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Czo1UciGomZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cbcd354a-5943-46c4-c8cf-b1c475df0131"
      },
      "source": [
        "from models.r21d.extract_r21d import ExtractR21D\n",
        "from utils.utils import build_cfg_path\n",
        "from omegaconf import OmegaConf\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMwy01A8w9nP"
      },
      "source": [
        "feature_type = 'r21d'\n",
        "\n",
        "args = OmegaConf.load(build_cfg_path(feature_type))\n",
        "# args.video_paths = ['./sample/v_GGSY1Qvo990.mp4']\n",
        "args.model_name = 'r2plus1d_18_16_kinetics'\n",
        "extractor = ExtractR21D(args)\n",
        "\n",
        "#the ./sample is where the videos should be allocated at\n",
        "for i,dir in enumerate(os.listdir('./sample')):\n",
        "  if dir.endswith('.wav'):\n",
        "    feature_dict = extractor.extract('./sample/' + dir)\n",
        "    #change the '' with the desired path to save at\n",
        "    np.save('./deep_{}_features.npy'.format(i), np.array(feature_dict, dtype=object), allow_pickle=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handcrafted features:\n",
        "\n",
        "Functions"
      ],
      "metadata": {
        "id": "XfuISCeqEGLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/open-mmlab/mmaction2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWFf8w3B2mz0",
        "outputId": "90273f3c-fdb8-4134-9457-932352c9670f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mmaction2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mmaction2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvfdK2FA20Vy",
        "outputId": "91c4d62b-c2da-4d13-d278-542551ff7705"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mmaction2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -v -e .\n",
        "!pip install -U openmim\n",
        "!mim install mmcv"
      ],
      "metadata": {
        "id": "yKBTInE223O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mmaction.utils import register_all_modules\n",
        "\n",
        "register_all_modules(init_default_scope=True)"
      ],
      "metadata": {
        "id": "hxw0MoshGeXZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mmcv\n",
        "import decord\n",
        "import numpy as np\n",
        "from mmcv.transforms import TRANSFORMS, BaseTransform, to_tensor\n",
        "from mmaction.structures import ActionDataSample\n",
        "\n",
        "\n",
        "@TRANSFORMS.register_module()\n",
        "class VideoInit(BaseTransform):\n",
        "    def transform(self, results):\n",
        "        container = decord.VideoReader(results['filename'])\n",
        "        results['total_frames'] = len(container)\n",
        "        results['video_reader'] = container\n",
        "        return results\n",
        "\n",
        "\n",
        "@TRANSFORMS.register_module()\n",
        "class VideoSample(BaseTransform):\n",
        "    def __init__(self, clip_len, num_clips, test_mode=False):\n",
        "        self.clip_len = clip_len\n",
        "        self.num_clips = num_clips\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "    def transform(self, results):\n",
        "        total_frames = results['total_frames']\n",
        "        interval = total_frames // self.clip_len\n",
        "\n",
        "        if self.test_mode:\n",
        "            # Make the sampling during testing deterministic\n",
        "            np.random.seed(42)\n",
        "\n",
        "        inds_of_all_clips = []\n",
        "        for i in range(self.num_clips):\n",
        "            bids = np.arange(self.clip_len) * interval\n",
        "            offset = np.random.randint(interval, size=bids.shape)\n",
        "            inds = bids + offset\n",
        "            inds_of_all_clips.append(inds)\n",
        "\n",
        "        results['frame_inds'] = np.concatenate(inds_of_all_clips)\n",
        "        results['clip_len'] = self.clip_len\n",
        "        results['num_clips'] = self.num_clips\n",
        "        return results\n",
        "\n",
        "\n",
        "@TRANSFORMS.register_module()\n",
        "class VideoDecode(BaseTransform):\n",
        "    def transform(self, results):\n",
        "        frame_inds = results['frame_inds']\n",
        "        container = results['video_reader']\n",
        "\n",
        "        imgs = container.get_batch(frame_inds).asnumpy()\n",
        "        imgs = list(imgs)\n",
        "\n",
        "        results['video_reader'] = None\n",
        "        del container\n",
        "\n",
        "        results['imgs'] = imgs\n",
        "        results['img_shape'] = imgs[0].shape[:2]\n",
        "        return results\n",
        "\n",
        "\n",
        "@TRANSFORMS.register_module()\n",
        "class VideoResize(BaseTransform):\n",
        "    def __init__(self, r_size):\n",
        "        self.r_size = (np.inf, r_size)\n",
        "\n",
        "    def transform(self, results):\n",
        "        img_h, img_w = results['img_shape']\n",
        "        new_w, new_h = mmcv.rescale_size((img_w, img_h), self.r_size)\n",
        "\n",
        "        imgs = [mmcv.imresize(img, (new_w, new_h))\n",
        "                for img in results['imgs']]\n",
        "        results['imgs'] = imgs\n",
        "        results['img_shape'] = imgs[0].shape[:2]\n",
        "        return results\n",
        "\n",
        "\n",
        "@TRANSFORMS.register_module()\n",
        "class VideoCrop(BaseTransform):\n",
        "    def __init__(self, c_size):\n",
        "        self.c_size = c_size\n",
        "\n",
        "    def transform(self, results):\n",
        "        img_h, img_w = results['img_shape']\n",
        "        center_x, center_y = img_w // 2, img_h // 2\n",
        "        x1, x2 = center_x - self.c_size // 2, center_x + self.c_size // 2\n",
        "        y1, y2 = center_y - self.c_size // 2, center_y + self.c_size // 2\n",
        "        imgs = [img[y1:y2, x1:x2] for img in results['imgs']]\n",
        "        results['imgs'] = imgs\n",
        "        results['img_shape'] = imgs[0].shape[:2]\n",
        "        return results\n",
        "\n",
        "\n",
        "@TRANSFORMS.register_module()\n",
        "class VideoFormat(BaseTransform):\n",
        "    def transform(self, results):\n",
        "        num_clips = results['num_clips']\n",
        "        clip_len = results['clip_len']\n",
        "        imgs = results['imgs']\n",
        "\n",
        "        # [num_clips*clip_len, H, W, C]\n",
        "        imgs = np.array(imgs)\n",
        "        # [num_clips, clip_len, H, W, C]\n",
        "        imgs = imgs.reshape((num_clips, clip_len) + imgs.shape[1:])\n",
        "        # [num_clips, C, clip_len, H, W]\n",
        "        imgs = imgs.transpose(0, 4, 1, 2, 3)\n",
        "\n",
        "        results['imgs'] = imgs\n",
        "        return results\n",
        "\n",
        "\n",
        "@TRANSFORMS.register_module()\n",
        "class VideoPack(BaseTransform):\n",
        "    def __init__(self, meta_keys=('img_shape', 'num_clips', 'clip_len')):\n",
        "        self.meta_keys = meta_keys\n",
        "\n",
        "    def transform(self, results):\n",
        "        packed_results = dict()\n",
        "        inputs = to_tensor(results['imgs'])\n",
        "        data_sample = ActionDataSample().set_gt_labels(results['label'])\n",
        "        metainfo = {k: results[k] for k in self.meta_keys if k in results}\n",
        "        data_sample.set_metainfo(metainfo)\n",
        "        packed_results['inputs'] = inputs\n",
        "        packed_results['data_samples'] = data_sample\n",
        "        return packed_results"
      ],
      "metadata": {
        "id": "xrvg0crJGhfj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "from mmengine.dataset import Compose\n",
        "\n",
        "pipeline_cfg = [\n",
        "    dict(type='VideoInit'),\n",
        "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
        "    dict(type='VideoDecode'),\n",
        "    dict(type='VideoResize', r_size=256),\n",
        "    dict(type='VideoCrop', c_size=224),\n",
        "    dict(type='VideoFormat'),\n",
        "    dict(type='VideoPack')\n",
        "]\n",
        "\n",
        "pipeline = Compose(pipeline_cfg)\n",
        "data_prefix = 'data/kinetics400_tiny/train'\n",
        "results = dict(filename=osp.join(data_prefix, 'D32_1gwq35E.mp4'), label=0)\n",
        "packed_results = pipeline(results)\n",
        "\n",
        "inputs = packed_results['inputs']\n",
        "data_sample = packed_results['data_samples']\n",
        "\n",
        "print('shape of the inputs: ', inputs.shape)\n",
        "\n",
        "# Get metainfo of the inputs\n",
        "print('image_shape: ', data_sample.img_shape)\n",
        "print('num_clips: ', data_sample.num_clips)\n",
        "print('clip_len: ', data_sample.clip_len)\n",
        "\n",
        "# Get label of the inputs\n",
        "print('label: ', data_sample.gt_labels.item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNrMWeGEHAIh",
        "outputId": "e65f8b49-e6a1-473b-8ec2-4c87f0635a44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the inputs:  torch.Size([1, 3, 16, 224, 224])\n",
            "image_shape:  (224, 224)\n",
            "num_clips:  1\n",
            "clip_len:  16\n",
            "label:  tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "from mmengine.fileio import list_from_file\n",
        "from mmengine.dataset import BaseDataset\n",
        "from mmaction.registry import DATASETS\n",
        "\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class DatasetZelda(BaseDataset):\n",
        "    def __init__(self, ann_file, pipeline, data_root, data_prefix=dict(video=''),\n",
        "                 test_mode=False, modality='RGB', **kwargs):\n",
        "        self.modality = modality\n",
        "        super(DatasetZelda, self).__init__(ann_file=ann_file, pipeline=pipeline, data_root=data_root,\n",
        "                                           data_prefix=data_prefix, test_mode=test_mode,\n",
        "                                           **kwargs)\n",
        "\n",
        "    def load_data_list(self):\n",
        "        data_list = []\n",
        "        fin = list_from_file(self.ann_file)\n",
        "        for line in fin:\n",
        "            line_split = line.strip().split()\n",
        "            filename, label = line_split\n",
        "            label = int(label)\n",
        "            filename = osp.join(self.data_prefix['video'], filename)\n",
        "            data_list.append(dict(filename=filename, label=label))\n",
        "        return data_list\n",
        "\n",
        "    def get_data_info(self, idx: int) -> dict:\n",
        "        data_info = super().get_data_info(idx)\n",
        "        data_info['modality'] = self.modality\n",
        "        return data_info"
      ],
      "metadata": {
        "id": "Y9e8L_06HE7c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mmaction.registry import DATASETS\n",
        "\n",
        "train_pipeline_cfg = [\n",
        "    dict(type='VideoInit'),\n",
        "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
        "    dict(type='VideoDecode'),\n",
        "    dict(type='VideoResize', r_size=256),\n",
        "    dict(type='VideoCrop', c_size=224),\n",
        "    dict(type='VideoFormat'),\n",
        "    dict(type='VideoPack')\n",
        "]\n",
        "\n",
        "val_pipeline_cfg = [\n",
        "    dict(type='VideoInit'),\n",
        "    dict(type='VideoSample', clip_len=16, num_clips=5, test_mode=True),\n",
        "    dict(type='VideoDecode'),\n",
        "    dict(type='VideoResize', r_size=256),\n",
        "    dict(type='VideoCrop', c_size=224),\n",
        "    dict(type='VideoFormat'),\n",
        "    dict(type='VideoPack')\n",
        "]\n",
        "\n",
        "train_dataset_cfg = dict(\n",
        "    type='DatasetZelda',\n",
        "    ann_file='kinetics_tiny_train_video.txt',\n",
        "    pipeline=train_pipeline_cfg,\n",
        "    data_root='data/kinetics400_tiny/',\n",
        "    data_prefix=dict(video='train'))\n",
        "\n",
        "val_dataset_cfg = dict(\n",
        "    type='DatasetZelda',\n",
        "    ann_file='kinetics_tiny_val_video.txt',\n",
        "    pipeline=val_pipeline_cfg,\n",
        "    data_root='data/kinetics400_tiny/',\n",
        "    data_prefix=dict(video='val'))\n",
        "\n",
        "train_dataset = DATASETS.build(train_dataset_cfg)\n",
        "\n",
        "packed_results = train_dataset[0]\n",
        "\n",
        "inputs = packed_results['inputs']\n",
        "data_sample = packed_results['data_samples']\n",
        "\n",
        "print('shape of the inputs: ', inputs.shape)\n",
        "\n",
        "# Get metainfo of the inputs\n",
        "print('image_shape: ', data_sample.img_shape)\n",
        "print('num_clips: ', data_sample.num_clips)\n",
        "print('clip_len: ', data_sample.clip_len)\n",
        "\n",
        "# Get label of the inputs\n",
        "print('label: ', data_sample.gt_labels.item)\n",
        "\n",
        "from mmengine.runner import Runner\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataloader_cfg = dict(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=0,\n",
        "    persistent_workers=False,\n",
        "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
        "    dataset=train_dataset_cfg)\n",
        "\n",
        "val_dataloader_cfg = dict(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=0,\n",
        "    persistent_workers=False,\n",
        "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
        "    dataset=val_dataset_cfg)\n",
        "\n",
        "train_data_loader = Runner.build_dataloader(dataloader=train_dataloader_cfg)\n",
        "val_data_loader = Runner.build_dataloader(dataloader=val_dataloader_cfg)\n",
        "\n",
        "batched_packed_results = next(iter(train_data_loader))\n",
        "\n",
        "batched_inputs = batched_packed_results['inputs']\n",
        "batched_data_sample = batched_packed_results['data_samples']\n",
        "\n",
        "assert len(batched_inputs) == BATCH_SIZE\n",
        "assert len(batched_data_sample) == BATCH_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZYLk5SvHHzc",
        "outputId": "22fdf2a4-0fc3-403a-dfbd-089270cd532b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the inputs:  torch.Size([1, 3, 16, 224, 224])\n",
            "image_shape:  (224, 224)\n",
            "num_clips:  1\n",
            "clip_len:  16\n",
            "label:  tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from mmengine.model import BaseDataPreprocessor, stack_batch\n",
        "from mmaction.registry import MODELS\n",
        "\n",
        "\n",
        "@MODELS.register_module()\n",
        "class DataPreprocessorZelda(BaseDataPreprocessor):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer(\n",
        "            'mean',\n",
        "            torch.tensor(mean, dtype=torch.float32).view(-1, 1, 1, 1),\n",
        "            False)\n",
        "        self.register_buffer(\n",
        "            'std',\n",
        "            torch.tensor(std, dtype=torch.float32).view(-1, 1, 1, 1),\n",
        "            False)\n",
        "\n",
        "    def forward(self, data, training=False):\n",
        "        data = self.cast_data(data)\n",
        "        inputs = data['inputs']\n",
        "        batch_inputs = stack_batch(inputs)  # Batching\n",
        "        batch_inputs = (batch_inputs - self.mean) / self.std  # Normalization\n",
        "        data['inputs'] = batch_inputs\n",
        "        return data"
      ],
      "metadata": {
        "id": "828ZAOUVHKgg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mmaction.registry import MODELS\n",
        "\n",
        "data_preprocessor_cfg = dict(\n",
        "    type='DataPreprocessorZelda',\n",
        "    mean=[123.675, 116.28, 103.53],\n",
        "    std=[58.395, 57.12, 57.375])\n",
        "\n",
        "data_preprocessor = MODELS.build(data_preprocessor_cfg)\n",
        "\n",
        "preprocessed_inputs = data_preprocessor(batched_packed_results)\n",
        "print(preprocessed_inputs['inputs'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF6yMfVUHNaX",
        "outputId": "ff358442-1dc6-4d84-d54d-5a5f1a3f962f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 3, 16, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from mmengine.model import BaseModel, BaseModule, Sequential\n",
        "from mmengine.structures import LabelData\n",
        "from mmaction.registry import MODELS\n",
        "\n",
        "\n",
        "@MODELS.register_module()\n",
        "class BackBoneZelda(BaseModule):\n",
        "    def __init__(self, init_cfg=None):\n",
        "        if init_cfg is None:\n",
        "            init_cfg = [dict(type='Kaiming', layer='Conv3d', mode='fan_out', nonlinearity=\"relu\"),\n",
        "                        dict(type='Constant', layer='BatchNorm3d', val=1, bias=0)]\n",
        "\n",
        "        super(BackBoneZelda, self).__init__(init_cfg=init_cfg)\n",
        "\n",
        "        self.conv1 = Sequential(nn.Conv3d(3, 64, kernel_size=(3, 7, 7),\n",
        "                                          stride=(1, 2, 2), padding=(1, 3, 3)),\n",
        "                                nn.BatchNorm3d(64), nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2),\n",
        "                                    padding=(0, 1, 1))\n",
        "\n",
        "        self.conv = Sequential(nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "                               nn.BatchNorm3d(128), nn.ReLU())\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        # imgs: [batch_size*num_views, 3, T, H, W]\n",
        "        # features: [batch_size*num_views, 128, T/2, H//8, W//8]\n",
        "        features = self.conv(self.maxpool(self.conv1(imgs)))\n",
        "        return features\n",
        "\n",
        "\n",
        "@MODELS.register_module()\n",
        "class ClsHeadZelda(BaseModule):\n",
        "    def __init__(self, num_classes, in_channels, dropout=0.5, average_clips='prob', init_cfg=None):\n",
        "        if init_cfg is None:\n",
        "            init_cfg = dict(type='Normal', layer='Linear', std=0.01)\n",
        "\n",
        "        super(ClsHeadZelda, self).__init__(init_cfg=init_cfg)\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.in_channels = in_channels\n",
        "        self.average_clips = average_clips\n",
        "\n",
        "        if dropout != 0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "        self.fc = nn.Linear(self.in_channels, self.num_classes)\n",
        "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, T, H, W = x.shape\n",
        "        x = self.pool(x)\n",
        "        x = x.view(N, C)\n",
        "        assert x.shape[1] == self.in_channels\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        cls_scores = self.fc(x)\n",
        "        return cls_scores\n",
        "\n",
        "    def loss(self, feats, data_samples):\n",
        "        cls_scores = self(feats)\n",
        "        labels = torch.stack([x.gt_labels.item for x in data_samples])\n",
        "        labels = labels.squeeze()\n",
        "\n",
        "        if labels.shape == torch.Size([]):\n",
        "            labels = labels.unsqueeze(0)\n",
        "\n",
        "        loss_cls = self.loss_fn(cls_scores, labels)\n",
        "        return dict(loss_cls=loss_cls)\n",
        "\n",
        "    def predict(self, feats, data_samples):\n",
        "        cls_scores = self(feats)\n",
        "        num_views = cls_scores.shape[0] // len(data_samples)\n",
        "        # assert num_views == data_samples[0].num_clips\n",
        "        cls_scores = self.average_clip(cls_scores, num_views)\n",
        "\n",
        "        for ds, sc in zip(data_samples, cls_scores):\n",
        "            pred = LabelData(item=sc)\n",
        "            ds.pred_scores = pred\n",
        "        return data_samples\n",
        "\n",
        "    def average_clip(self, cls_scores, num_views):\n",
        "          if self.average_clips not in ['score', 'prob', None]:\n",
        "            raise ValueError(f'{self.average_clips} is not supported. '\n",
        "                             f'Currently supported ones are '\n",
        "                             f'[\"score\", \"prob\", None]')\n",
        "\n",
        "          total_views = cls_scores.shape[0]\n",
        "          cls_scores = cls_scores.view(total_views // num_views, num_views, -1)\n",
        "\n",
        "          if self.average_clips is None:\n",
        "              return cls_scores\n",
        "          elif self.average_clips == 'prob':\n",
        "              cls_scores = F.softmax(cls_scores, dim=2).mean(dim=1)\n",
        "          elif self.average_clips == 'score':\n",
        "              cls_scores = cls_scores.mean(dim=1)\n",
        "\n",
        "          return cls_scores\n",
        "\n",
        "\n",
        "@MODELS.register_module()\n",
        "class RecognizerZelda(BaseModel):\n",
        "    def __init__(self, backbone, cls_head, data_preprocessor):\n",
        "        super().__init__(data_preprocessor=data_preprocessor)\n",
        "\n",
        "        self.backbone = MODELS.build(backbone)\n",
        "        self.cls_head = MODELS.build(cls_head)\n",
        "\n",
        "    def extract_feat(self, inputs):\n",
        "        inputs = inputs.view((-1, ) + inputs.shape[2:])\n",
        "        return self.backbone(inputs)\n",
        "\n",
        "    def loss(self, inputs, data_samples):\n",
        "        feats = self.extract_feat(inputs)\n",
        "        loss = self.cls_head.loss(feats, data_samples)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, inputs, data_samples):\n",
        "        feats = self.extract_feat(inputs)\n",
        "        predictions = self.cls_head.predict(feats, data_samples)\n",
        "        return predictions\n",
        "\n",
        "    def forward(self, inputs, data_samples=None, mode='tensor'):\n",
        "        if mode == 'tensor':\n",
        "            return self.extract_feat(inputs)\n",
        "        elif mode == 'loss':\n",
        "            return self.loss(inputs, data_samples)\n",
        "        elif mode == 'predict':\n",
        "            return self.predict(inputs, data_samples)\n",
        "        else:\n",
        "            raise RuntimeError(f'Invalid mode: {mode}')"
      ],
      "metadata": {
        "id": "91_sHM0wHRKH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "from mmaction.registry import MODELS\n",
        "\n",
        "model_cfg = dict(\n",
        "    type='RecognizerZelda',\n",
        "    backbone=dict(type='BackBoneZelda'),\n",
        "    cls_head=dict(\n",
        "        type='ClsHeadZelda',\n",
        "        num_classes=2,\n",
        "        in_channels=128,\n",
        "        average_clips='prob'),\n",
        "    data_preprocessor = dict(\n",
        "        type='DataPreprocessorZelda',\n",
        "        mean=[123.675, 116.28, 103.53],\n",
        "        std=[58.395, 57.12, 57.375]))\n",
        "\n",
        "model = MODELS.build(model_cfg)\n",
        "\n",
        "# Train\n",
        "model.train()\n",
        "model.init_weights()\n",
        "data_batch_train = copy.deepcopy(batched_packed_results)\n",
        "data = model.data_preprocessor(data_batch_train, training=True)\n",
        "loss = model(**data, mode='loss')\n",
        "print('loss dict: ', loss)\n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    data_batch_test = copy.deepcopy(batched_packed_results)\n",
        "    data = model.data_preprocessor(data_batch_test, training=False)\n",
        "    predictions = model(**data, mode='predict')\n",
        "print('Label of Sample[0]', predictions[0].gt_labels.item)\n",
        "print('Scores of Sample[0]', predictions[0].pred_scores.item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_gjwWfNHSfl",
        "outputId": "5e36f959-6084-45cd-baf2-c2234c5c1239"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv1.0.weight - torch.Size([64, 3, 3, 7, 7]): \n",
            "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv1.0.bias - torch.Size([64]): \n",
            "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv1.1.weight - torch.Size([64]): \n",
            "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv1.1.bias - torch.Size([64]): \n",
            "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv.0.weight - torch.Size([128, 64, 3, 3, 3]): \n",
            "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv.0.bias - torch.Size([128]): \n",
            "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv.1.weight - torch.Size([128]): \n",
            "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "backbone.conv.1.bias - torch.Size([128]): \n",
            "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "cls_head.fc.weight - torch.Size([2, 128]): \n",
            "NormalInit: mean=0, std=0.01, bias=0 \n",
            " \n",
            "07/10 18:43:08 - mmengine - INFO - \n",
            "cls_head.fc.bias - torch.Size([2]): \n",
            "NormalInit: mean=0, std=0.01, bias=0 \n",
            " \n",
            "loss dict:  {'loss_cls': tensor(0.7162, grad_fn=<NllLossBackward0>)}\n",
            "Label of Sample[0] tensor([0])\n",
            "Scores of Sample[0] tensor([0.5043, 0.4957])\n"
          ]
        }
      ]
    }
  ]
}